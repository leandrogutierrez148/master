{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgIc4QQ7skh5"
      },
      "source": [
        "# Neural Style Transfer\n",
        "En el pdf de la actividad se describe brevemente el método de generación de imágenes **Neural Style Transfer**. No se pretende adquirir un conocimiento profundo de este método sino utilizar la práctica para adquirir un **conocimiento intuitivo de las Redes Neuronales Convolucionales como extractores de características**.\n",
        "\n",
        "En primer lugar, importamos PyTorch junto con otros paquetes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu_4GvC9mGtI"
      },
      "source": [
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import requests\n",
        "from torchvision import transforms, models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OzM1NgwyfHw"
      },
      "source": [
        "## Transfer Learning: descargamos VGG19\n",
        "En esta actividad vamos a utilizar el primer bloque de VGG19, que sirve para **extraer las características** de la imagen de entrada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL5BdsTUl61l"
      },
      "source": [
        "# reutilizamos el modelo pre-entrenado VGG19\n",
        "vgg_all = models.vgg19(pretrained=True)\n",
        "\n",
        "# pero seleccionamos solo el primer bloque de la red que extrae features\n",
        "vgg_features = vgg_all.features\n",
        "\n",
        "# congelamos porque solo queremos optimizar la imagen target\n",
        "for param in vgg_features.parameters():\n",
        "    param.requires_grad_(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JrzTAcm1PjX"
      },
      "source": [
        "## VGG19 - Clasificador\n",
        "Aunque en esta práctica solo vamos a utilizar el bloque extractor de características, podemos aprovechar para echarle un vistazo al bloque de **clasificación**.\n",
        "\n",
        "Comprobamos que el segundo bloque, el clasificador, está compuesto por:\n",
        "* capas lineales (Linear)\n",
        "* funciones de activación ReLU\n",
        "* capas de dropout para evitar overfitting\n",
        "\n",
        "Si quisiéramos utilizar VGG19 como clasificador de un dataset de imágenes, deberíamos sustituir este bloque por otro similar con tantas salidas como clases haya en el dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ce64JM804NZ"
      },
      "source": [
        "vgg_classifier = vgg_all.classifier\n",
        "print(vgg_classifier)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wHE-E-p_mse"
      },
      "source": [
        "## Pasamos modelo a GPU si está disponible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gua1doWpNr7"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "vgg_features.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQI990h8NAMM"
      },
      "source": [
        "## load_image\n",
        "Función para cargar una imagen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1gzGEa5pfxW"
      },
      "source": [
        "def load_image(img_path, max_size=300, shape=None):\n",
        "    ''' Load in and transform an image, making sure the image\n",
        "       is <= max_size pixels in the x-y dims.'''\n",
        "    if \"http\" in img_path:\n",
        "        response = requests.get(img_path)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # large images will slow down processing\n",
        "    if max(image.size) > max_size:\n",
        "        size = max_size\n",
        "    else:\n",
        "        size = max(image.size)\n",
        "\n",
        "    if shape is not None:\n",
        "        size = shape\n",
        "\n",
        "    # VGG está normalizada con los valores de Imagenet: 0.406, 0.456, 0.485 en BGR, que invertimos en RGB.\n",
        "    # En rango 0-255 seria 103.94, 116.78, 123.68 en BGR\n",
        "    in_transform = transforms.Compose([\n",
        "                        transforms.Resize(size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                                             (0.229, 0.224, 0.225))])\n",
        "\n",
        "    # con :3 eliminamos el último canal de la primera dimensión, que es el alfa e indica la transparencia\n",
        "    # con unsqueeze(0) añadimos como primera dimensión el batch\n",
        "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
        "\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8TaKWkT_ezG"
      },
      "source": [
        "## Carga de las imágenes de contenido y estilo\n",
        "Además, cambiamos tamaño de la imagen de estilo a la de contenido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L61iF74pmJA"
      },
      "source": [
        "# cargamos las imágenes de contenido y estilo\n",
        "img_quinta = \"https://github.com/md-lorente/Master_BD_DS/blob/main/m%C3%B3dulo_7_aprendizaje_autom%C3%A1tico_para_machine_learning/quinta.jpeg?raw=true\"\n",
        "img_palamos = \"https://github.com/md-lorente/Master_BD_DS/blob/main/m%C3%B3dulo_7_aprendizaje_autom%C3%A1tico_para_machine_learning/playa_palamos.jpg\"\n",
        "img_soto = \"https://github.com/md-lorente/Master_BD_DS/blob/main/m%C3%B3dulo_7_aprendizaje_autom%C3%A1tico_para_machine_learning/soto_cruz_mazo.jpg?raw=true\"\n",
        "\n",
        "content = load_image(img_soto).to(device)\n",
        "\n",
        "img_van_gogh = \"https://github.com/md-lorente/Master_BD_DS/blob/main/m%C3%B3dulo_7_aprendizaje_autom%C3%A1tico_para_machine_learning/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg?raw=true\"\n",
        "img_kandinsky = \"https://github.com/md-lorente/Master_BD_DS/blob/main/m%C3%B3dulo_7_aprendizaje_autom%C3%A1tico_para_machine_learning/Vassily_Kandinsky,_1913_-_Composition_7.jpg?raw=true\"\n",
        "\n",
        "# cambia tamaño de la imagen de estilo: coge los dos últimos valores de shape de la imagen de contenido\n",
        "from time import sleep\n",
        "error = True\n",
        "while error == True:\n",
        "    try:\n",
        "      style = load_image(img_van_gogh, shape=content.shape[-2:]).to(device)\n",
        "      error = False\n",
        "    except:\n",
        "      print(\"retry\")\n",
        "      sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q25xz0-1Aebt"
      },
      "source": [
        "## im_convert\n",
        "Es una función de ayuda para pasar una imagen de Tensor a numpy y poderla visualizar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lRAgIyEpphk"
      },
      "source": [
        "def im_convert(tensor):\n",
        "\n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.numpy().squeeze()\n",
        "    image = image.transpose(1,2,0) # cambia ancho por alto\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsSp0-gIAwn2"
      },
      "source": [
        "## Veamos las imágenes de contenido y estilo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsY1WrhopshE"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(style))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uxYiVdEA6w1"
      },
      "source": [
        "## Capas de vgg_features\n",
        "Veamos los identificadores de capas del extractor de características, vgg_features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lsXC66Mqrca"
      },
      "source": [
        "print(vgg_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6THiTFjcBTHk"
      },
      "source": [
        "## Extracción de características\n",
        "La función get_features se utiliza para extraer las características de las imágenes de contenido y estilo.\n",
        "\n",
        "En primer lugar, filtramos las capas que vamos a tener en cuenta para la extracción de características según se indica en el artículo de referencia. Utilizaremos varias capas para el estilo pero solo una (conv4_2) para el contenido.\n",
        "\n",
        "A continuación, extraemos las características de la imagen para las capas seleccionadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTAWVSuGpvil"
      },
      "source": [
        "def get_features(image, model, layers=None):\n",
        "\n",
        "    if layers is None:\n",
        "        layers = {'0': 'conv1_1',\n",
        "                  '5': 'conv2_1',\n",
        "                  '10': 'conv3_1',\n",
        "                  '19': 'conv4_1',\n",
        "                  '21': 'conv4_2',  ## contenido\n",
        "                  '28': 'conv5_1'}\n",
        "\n",
        "    features = {}\n",
        "    x = image\n",
        "    # model._modules es un diccionario con todas las capas\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x) # la imagen se va transformando en cada capa...\n",
        "        if name in layers: # ...pero solo extraemos las características de las capas seleccionadas\n",
        "            features[layers[name]] = x\n",
        "\n",
        "    return features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1db8kPVrD_nI"
      },
      "source": [
        "## Matriz Gram\n",
        "El método Style Transfer utiliza la matriz Gram o Gramian para procesar las características.\n",
        "\n",
        "Ver https://en.wikipedia.org/wiki/Gramian_matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgfGTc7Rp1BX"
      },
      "source": [
        "def gram_matrix(tensor):\n",
        "\n",
        "    # obtenemos batch_size, depth, height y width del Tensor\n",
        "    b, d, h, w = tensor.size()\n",
        "\n",
        "    # convertimos el tensor a 2D multiplicando dimensiones entre sí\n",
        "    tensor = tensor.view(b * d, h * w)\n",
        "\n",
        "    # calculamos la matriz gram multiplicando el tensor por su transpuesto\n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "\n",
        "    return gram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqDblvrLGl3T"
      },
      "source": [
        "## Definimos los parámetros a probar\n",
        "* Definimos los pesos de las capas de estilo. Excluimos conv4_2 porque es la capa de la que extraemos el contenido.\n",
        "* Definimos valores de los pesos de contenido y estilo para la función de pérdida (content_weight, style_weight).\n",
        "* Definimos la tasa de aprendizaje.\n",
        "* Definimos el número de iteraciones (steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymQWkw5gq2YK"
      },
      "source": [
        "style_weights = {'conv1_1': 1.,\n",
        "                 'conv2_1': 0.75,\n",
        "                 'conv3_1': 0.2,\n",
        "                 'conv4_1': 0.2,\n",
        "                 'conv5_1': 0.2}\n",
        "\n",
        "content_weight = 1  # alpha\n",
        "style_weight = 1e6  # beta\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "steps = 500 # número de iteraciones\n",
        "\n",
        "# definimos cada cuántas imágenes queremos hacer una visualización intermedia\n",
        "show_every = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD8toeUV0blh"
      },
      "source": [
        "## Inicialización y entrenamiento del modelo\n",
        "Tareas:\n",
        "* definimos las capas del nuevo modelo\n",
        "* inicializamos la imagen generada con la imagen de contenido\n",
        "* definimos el optimizador\n",
        "* modificamos la imagen generada mediante la función de pérdida, compuesta por la suma de la pérdida de estilo y la de contenido ponderadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMwlUS1_q6zS"
      },
      "source": [
        "# extraemos características de contenido y estilo de las imágenes originales\n",
        "content_features = get_features(content, vgg_features)\n",
        "style_features = get_features(style, vgg_features)\n",
        "\n",
        "# calculamos la matriz gram de cada capa de estilo\n",
        "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "# creamos una tercera imagen \"target\" que se va a ir modificando\n",
        "# La inicializamos con la imagen de contenido...\n",
        "# ...y le vamos cambiando el estilo\n",
        "target = content.clone().requires_grad_(True).to(device)\n",
        "\n",
        "# definimos el optimizador sobre los parámetros de la imagen target\n",
        "optimizer = optim.Adam([target], lr=learning_rate)\n",
        "\n",
        "for ii in range(1, steps+1):\n",
        "\n",
        "    # extraemos las características de la imagen generada\n",
        "    target_features = get_features(target, vgg_features)\n",
        "\n",
        "    # definimos la función de pérdida de contenido\n",
        "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
        "\n",
        "    # definimos la función de pérdida de estilo como suma de la pérdida de cada capa\n",
        "    style_loss = 0\n",
        "    for layer in style_weights:\n",
        "        target_feature = target_features[layer]\n",
        "        target_gram = gram_matrix(target_feature)\n",
        "        _, d, h, w = target_feature.shape\n",
        "        style_gram = style_grams[layer]\n",
        "        # calculamos la \"pérdida de estilo\" de la capa actual y la ponderamos\n",
        "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
        "        # la añadimos a la pérdida de estilo\n",
        "        style_loss += layer_style_loss / (d * h * w)\n",
        "\n",
        "    # calculamos la pérdida total\n",
        "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "\n",
        "    # actualizamos la imagen generada mediante el optimizador\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # mostramos las imágenes intermedias y su pérdida\n",
        "    if  ii % show_every == 0:\n",
        "        print('Pérdida total: ', total_loss.item())\n",
        "        plt.imshow(im_convert(target))\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCRtb4ihHsu-"
      },
      "source": [
        "## Veamos la imagen de contenido y la generada (final)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIZSqC5irEnz"
      },
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtfHat5x-5Vo"
      },
      "source": [
        "## Almacenamiento en local\n",
        "Por último, si queremos conservar la imagen resultante, podemos bajarla imagen a nuestro ordenador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK15UM08vXaj"
      },
      "source": [
        "from google.colab import files\n",
        "target_path = \"content_style.jpg\"\n",
        "\n",
        "target_im = im_convert(target)\n",
        "target_im = target_im * 255\n",
        "target_im = target_im.astype('uint8')\n",
        "Image.fromarray(target_im).save(target_path)\n",
        "\n",
        "files.download(target_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh1g5BQp3SlY"
      },
      "source": [
        "# Preguntas\n",
        "### 1 - ¿Cuántas capas lineales contiene el clasificador de VGG19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH1fEOz-3lZA"
      },
      "source": [
        "### 2 - ¿Cuántas clases distintas puede reconocer VGG19 en su configuración original?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTDHBPPU3lax"
      },
      "source": [
        "### 3 - Indica los identificadores de capas convolucionales a continuación de capas de max pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB9Wt6o04ghH"
      },
      "source": [
        "### 4 - ¿Qué influencia tienen los cambios solicitados (content_weight, style_weight, learning rate, steps) en las imágenes intermedias y en la imagen resultante?"
      ]
    }
  ]
}